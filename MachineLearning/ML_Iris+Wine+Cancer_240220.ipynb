{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "404b18ce",
   "metadata": {},
   "source": [
    "# Handwritting Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb36bc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8a0e4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel_0_0</th>\n",
       "      <th>pixel_0_1</th>\n",
       "      <th>pixel_0_2</th>\n",
       "      <th>pixel_0_3</th>\n",
       "      <th>pixel_0_4</th>\n",
       "      <th>pixel_0_5</th>\n",
       "      <th>pixel_0_6</th>\n",
       "      <th>pixel_0_7</th>\n",
       "      <th>pixel_1_0</th>\n",
       "      <th>pixel_1_1</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel_6_6</th>\n",
       "      <th>pixel_6_7</th>\n",
       "      <th>pixel_7_0</th>\n",
       "      <th>pixel_7_1</th>\n",
       "      <th>pixel_7_2</th>\n",
       "      <th>pixel_7_3</th>\n",
       "      <th>pixel_7_4</th>\n",
       "      <th>pixel_7_5</th>\n",
       "      <th>pixel_7_6</th>\n",
       "      <th>pixel_7_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel_0_0  pixel_0_1  pixel_0_2  pixel_0_3  pixel_0_4  pixel_0_5  \\\n",
       "0           0.0        0.0        5.0       13.0        9.0        1.0   \n",
       "1           0.0        0.0        0.0       12.0       13.0        5.0   \n",
       "2           0.0        0.0        0.0        4.0       15.0       12.0   \n",
       "3           0.0        0.0        7.0       15.0       13.0        1.0   \n",
       "4           0.0        0.0        0.0        1.0       11.0        0.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1792        0.0        0.0        4.0       10.0       13.0        6.0   \n",
       "1793        0.0        0.0        6.0       16.0       13.0       11.0   \n",
       "1794        0.0        0.0        1.0       11.0       15.0        1.0   \n",
       "1795        0.0        0.0        2.0       10.0        7.0        0.0   \n",
       "1796        0.0        0.0       10.0       14.0        8.0        1.0   \n",
       "\n",
       "      pixel_0_6  pixel_0_7  pixel_1_0  pixel_1_1  ...  pixel_6_6  pixel_6_7  \\\n",
       "0           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "2           0.0        0.0        0.0        0.0  ...        5.0        0.0   \n",
       "3           0.0        0.0        0.0        8.0  ...        9.0        0.0   \n",
       "4           0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "...         ...        ...        ...        ...  ...        ...        ...   \n",
       "1792        0.0        0.0        0.0        1.0  ...        4.0        0.0   \n",
       "1793        1.0        0.0        0.0        0.0  ...        1.0        0.0   \n",
       "1794        0.0        0.0        0.0        0.0  ...        0.0        0.0   \n",
       "1795        0.0        0.0        0.0        0.0  ...        2.0        0.0   \n",
       "1796        0.0        0.0        0.0        2.0  ...        8.0        0.0   \n",
       "\n",
       "      pixel_7_0  pixel_7_1  pixel_7_2  pixel_7_3  pixel_7_4  pixel_7_5  \\\n",
       "0           0.0        0.0        6.0       13.0       10.0        0.0   \n",
       "1           0.0        0.0        0.0       11.0       16.0       10.0   \n",
       "2           0.0        0.0        0.0        3.0       11.0       16.0   \n",
       "3           0.0        0.0        7.0       13.0       13.0        9.0   \n",
       "4           0.0        0.0        0.0        2.0       16.0        4.0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1792        0.0        0.0        2.0       14.0       15.0        9.0   \n",
       "1793        0.0        0.0        6.0       16.0       14.0        6.0   \n",
       "1794        0.0        0.0        2.0        9.0       13.0        6.0   \n",
       "1795        0.0        0.0        5.0       12.0       16.0       12.0   \n",
       "1796        0.0        1.0        8.0       12.0       14.0       12.0   \n",
       "\n",
       "      pixel_7_6  pixel_7_7  \n",
       "0           0.0        0.0  \n",
       "1           0.0        0.0  \n",
       "2           9.0        0.0  \n",
       "3           0.0        0.0  \n",
       "4           0.0        0.0  \n",
       "...         ...        ...  \n",
       "1792        0.0        0.0  \n",
       "1793        0.0        0.0  \n",
       "1794        0.0        0.0  \n",
       "1795        0.0        0.0  \n",
       "1796        1.0        0.0  \n",
       "\n",
       "[1797 rows x 64 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_data = digits.data\n",
    "digits_label = digits.target\n",
    "\n",
    "digits_label\n",
    "\n",
    "df = pd.DataFrame(digits.data, columns=digits.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "057d5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = digits.target\n",
    "df.drop('label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b14fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        33\n",
      "           1       0.97      1.00      0.98        28\n",
      "           2       1.00      1.00      1.00        33\n",
      "           3       1.00      0.94      0.97        34\n",
      "           4       0.98      1.00      0.99        46\n",
      "           5       0.94      0.96      0.95        47\n",
      "           6       0.97      0.97      0.97        35\n",
      "           7       0.97      0.97      0.97        34\n",
      "           8       0.97      0.97      0.97        30\n",
      "           9       0.95      0.95      0.95        40\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.97      0.97      0.97       360\n",
      "weighted avg       0.97      0.97      0.97       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, digits.target, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62cf3a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       1.00      1.00      1.00        28\n",
      "           2       1.00      1.00      1.00        33\n",
      "           3       1.00      1.00      1.00        34\n",
      "           4       1.00      1.00      1.00        46\n",
      "           5       0.98      0.98      0.98        47\n",
      "           6       0.97      1.00      0.99        35\n",
      "           7       0.97      0.97      0.97        34\n",
      "           8       1.00      0.97      0.98        30\n",
      "           9       0.95      0.95      0.95        40\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.99      0.99      0.99       360\n",
      "weighted avg       0.99      0.99      0.99       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3b078b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.2s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.2s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.3s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.1s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.1s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.2s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.1s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.2s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.1s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.1s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.1s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.2s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.2s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.2s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.2s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.2s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.2s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.2s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.2s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.2s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.2s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.2s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.2s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.2s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.2s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.2s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.2s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.2s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.2s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
      "Best parameters found:  {'C': 0.1, 'gamma': 1, 'kernel': 'poly'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       1.00      1.00      1.00        28\n",
      "           2       1.00      1.00      1.00        33\n",
      "           3       0.97      1.00      0.99        34\n",
      "           4       1.00      1.00      1.00        46\n",
      "           5       0.98      0.98      0.98        47\n",
      "           6       0.97      1.00      0.99        35\n",
      "           7       0.97      0.97      0.97        34\n",
      "           8       1.00      0.97      0.98        30\n",
      "           9       0.97      0.95      0.96        40\n",
      "\n",
      "    accuracy                           0.99       360\n",
      "   macro avg       0.99      0.99      0.99       360\n",
      "weighted avg       0.99      0.99      0.99       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the parameter grid: \n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],  # Regularization parameter\n",
    "    'gamma': [1, 0.1, 0.01],  # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']  # Specifies the kernel type to be used in the algorithm\n",
    "}\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svc = SVC()\n",
    "\n",
    "# Initialize the GridSearchCV object with the SVM classifier, parameter grid, and number of folds for cross-validation\n",
    "grid_search = GridSearchCV(svc, param_grid, refit=True, verbose=2, cv=3)  # cv=3 means 3-fold cross-validation\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "pred = grid_search.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a4ec56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97        33\n",
      "           1       1.00      0.86      0.92        28\n",
      "           2       0.92      1.00      0.96        33\n",
      "           3       1.00      0.94      0.97        34\n",
      "           4       0.96      1.00      0.98        46\n",
      "           5       0.96      0.94      0.95        47\n",
      "           6       1.00      0.94      0.97        35\n",
      "           7       1.00      0.97      0.99        34\n",
      "           8       0.74      0.97      0.84        30\n",
      "           9       0.95      0.90      0.92        40\n",
      "\n",
      "    accuracy                           0.95       360\n",
      "   macro avg       0.95      0.95      0.95       360\n",
      "weighted avg       0.95      0.95      0.95       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SDG Classifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20c60f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        33\n",
      "           1       0.97      1.00      0.98        28\n",
      "           2       1.00      1.00      1.00        33\n",
      "           3       0.97      0.97      0.97        34\n",
      "           4       1.00      0.98      0.99        46\n",
      "           5       0.92      0.96      0.94        47\n",
      "           6       0.97      0.97      0.97        35\n",
      "           7       1.00      0.97      0.99        34\n",
      "           8       0.97      0.97      0.97        30\n",
      "           9       0.97      0.95      0.96        40\n",
      "\n",
      "    accuracy                           0.97       360\n",
      "   macro avg       0.98      0.98      0.98       360\n",
      "weighted avg       0.98      0.97      0.98       360\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=500)  # Increased max_iter for convergence, adjust as needed\n",
    "\n",
    "# Fit the model to your training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "accuracy_score(y_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f01f48e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: lbfgs_C=0.01, Accuracy: 0.975\n",
      "Config: lbfgs_C=0.1, Accuracy: 0.9722222222222222\n",
      "Config: lbfgs_C=1, Accuracy: 0.9722222222222222\n",
      "Config: lbfgs_C=10, Accuracy: 0.9722222222222222\n",
      "Config: lbfgs_C=100, Accuracy: 0.9722222222222222\n",
      "Config: liblinear_C=0.01, Accuracy: 0.9638888888888889\n",
      "Config: liblinear_C=0.1, Accuracy: 0.9611111111111111\n",
      "Config: liblinear_C=1, Accuracy: 0.9611111111111111\n",
      "Config: liblinear_C=10, Accuracy: 0.9555555555555556\n",
      "Config: liblinear_C=100, Accuracy: 0.9472222222222222\n"
     ]
    }
   ],
   "source": [
    "# GPT code for hyperparameter tuning\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the hyperparameters you want to test\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "solvers = ['lbfgs', 'liblinear']\n",
    "\n",
    "# Prepare a dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "for solver in solvers:\n",
    "    for C in C_values:\n",
    "        # Initialize the Logistic Regression model with current hyperparameters\n",
    "        model = LogisticRegression(C=C, solver=solver, max_iter=1000)\n",
    "        \n",
    "        # Fit the model to your training data\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test set\n",
    "        pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate the model using classification report\n",
    "        report = classification_report(y_test, pred, output_dict=True)\n",
    "        \n",
    "        # Store the results\n",
    "        results[f'{solver}_C={C}'] = report['accuracy']  # Example of storing accuracy; can store more metrics\n",
    "        \n",
    "# Display the results\n",
    "for config, accuracy in results.items():\n",
    "    print(f\"Config: {config}, Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235b4464",
   "metadata": {},
   "source": [
    "## Handwritting 회고\n",
    "- SVM 이 가장 성능이 좋았다. 0.99!\n",
    "- GridSearchCV 를 했는데, 어려운 문제가 아니었는지? 하이퍼파라미터 튜닝으로 성능이 확 뛰거나 하진 않았다\n",
    "- 평가 지표는 F1 score 를 사용하려고 했다. 아무래도 카테고리 10개이다보니 전반적인 성능을 보고싶었다. precision recall accuracy 모두 같이 가는 경향이 있었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e9fad",
   "metadata": {},
   "source": [
    "# Wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6146fb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wine = load_wine()\n",
    "\n",
    "wine.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ad917c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40e7472b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'malic_acid',\n",
       " 'ash',\n",
       " 'alcalinity_of_ash',\n",
       " 'magnesium',\n",
       " 'total_phenols',\n",
       " 'flavanoids',\n",
       " 'nonflavanoid_phenols',\n",
       " 'proanthocyanins',\n",
       " 'color_intensity',\n",
       " 'hue',\n",
       " 'od280/od315_of_diluted_wines',\n",
       " 'proline']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5830c919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((142, 13), (36, 13), (142,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d392f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "        1.065e+03],\n",
       "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
       "        1.050e+03],\n",
       "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
       "        1.185e+03],\n",
       "       ...,\n",
       "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "        8.350e+02],\n",
       "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "        8.400e+02],\n",
       "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "        5.600e+02]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bb5e641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  \n",
       "0                            3.92   1065.0  \n",
       "1                            3.40   1050.0  \n",
       "2                            3.17   1185.0  \n",
       "3                            3.45   1480.0  \n",
       "4                            2.93    735.0  \n",
       "..                            ...      ...  \n",
       "173                          1.74    740.0  \n",
       "174                          1.56    750.0  \n",
       "175                          1.56    835.0  \n",
       "176                          1.62    840.0  \n",
       "177                          1.60    560.0  \n",
       "\n",
       "[178 rows x 13 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2575962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>178.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000618</td>\n",
       "      <td>2.336348</td>\n",
       "      <td>2.366517</td>\n",
       "      <td>19.494944</td>\n",
       "      <td>99.741573</td>\n",
       "      <td>2.295112</td>\n",
       "      <td>2.029270</td>\n",
       "      <td>0.361854</td>\n",
       "      <td>1.590899</td>\n",
       "      <td>5.058090</td>\n",
       "      <td>0.957449</td>\n",
       "      <td>2.611685</td>\n",
       "      <td>746.893258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.811827</td>\n",
       "      <td>1.117146</td>\n",
       "      <td>0.274344</td>\n",
       "      <td>3.339564</td>\n",
       "      <td>14.282484</td>\n",
       "      <td>0.625851</td>\n",
       "      <td>0.998859</td>\n",
       "      <td>0.124453</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>2.318286</td>\n",
       "      <td>0.228572</td>\n",
       "      <td>0.709990</td>\n",
       "      <td>314.907474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.030000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>1.270000</td>\n",
       "      <td>278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.362500</td>\n",
       "      <td>1.602500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>1.742500</td>\n",
       "      <td>1.205000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>3.220000</td>\n",
       "      <td>0.782500</td>\n",
       "      <td>1.937500</td>\n",
       "      <td>500.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.050000</td>\n",
       "      <td>1.865000</td>\n",
       "      <td>2.360000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>2.355000</td>\n",
       "      <td>2.135000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>1.555000</td>\n",
       "      <td>4.690000</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>673.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13.677500</td>\n",
       "      <td>3.082500</td>\n",
       "      <td>2.557500</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.875000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>3.170000</td>\n",
       "      <td>985.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.830000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.230000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>3.880000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>3.580000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.710000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1680.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          alcohol  malic_acid         ash  alcalinity_of_ash   magnesium  \\\n",
       "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
       "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
       "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
       "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
       "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
       "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
       "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
       "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
       "\n",
       "       total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "count     178.000000  178.000000            178.000000       178.000000   \n",
       "mean        2.295112    2.029270              0.361854         1.590899   \n",
       "std         0.625851    0.998859              0.124453         0.572359   \n",
       "min         0.980000    0.340000              0.130000         0.410000   \n",
       "25%         1.742500    1.205000              0.270000         1.250000   \n",
       "50%         2.355000    2.135000              0.340000         1.555000   \n",
       "75%         2.800000    2.875000              0.437500         1.950000   \n",
       "max         3.880000    5.080000              0.660000         3.580000   \n",
       "\n",
       "       color_intensity         hue  od280/od315_of_diluted_wines      proline  \n",
       "count       178.000000  178.000000                    178.000000   178.000000  \n",
       "mean          5.058090    0.957449                      2.611685   746.893258  \n",
       "std           2.318286    0.228572                      0.709990   314.907474  \n",
       "min           1.280000    0.480000                      1.270000   278.000000  \n",
       "25%           3.220000    0.782500                      1.937500   500.500000  \n",
       "50%           4.690000    0.965000                      2.780000   673.500000  \n",
       "75%           6.200000    1.120000                      3.170000   985.000000  \n",
       "max          13.000000    1.710000                      4.000000  1680.000000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fb88666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f57dda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        14\n",
      "           1       0.93      1.00      0.97        14\n",
      "           2       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.98      0.98        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=500)  # Increased max_iter for convergence, adjust as needed\n",
    "\n",
    "# Fit the model to your training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "accuracy_score(y_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9401209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        14\n",
      "           1       1.00      0.21      0.35        14\n",
      "           2       0.37      0.88      0.52         8\n",
      "\n",
      "    accuracy                           0.64        36\n",
      "   macro avg       0.77      0.67      0.60        36\n",
      "weighted avg       0.83      0.64      0.61        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SDG Classifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7a7a99c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       0.73      0.79      0.76        14\n",
      "           2       0.57      0.50      0.53         8\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.77      0.76      0.76        36\n",
      "weighted avg       0.80      0.81      0.80        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc8aed",
   "metadata": {},
   "source": [
    "## Wine 회고\n",
    "- RandomForest 가 가장 성능이 좋았다. 첫 모델에서부터 1이 나와서 오류난줄 알았다\n",
    "- SVM 은 여기서는 F1 0.76 이었다. \n",
    "- 와인 모델별로 예측 정확도가 달라지는게 신기했다. 0번 모델은 맞추기가 쉬웠나"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8abeca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67190be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['class_0', 'class_1', 'class_2'], dtype='<U7')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a912b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(wine.frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92113a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68   \n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80   \n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59   \n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65   \n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            3.92   1065.0       0  \n",
       "1                            3.40   1050.0       0  \n",
       "2                            3.17   1185.0       0  \n",
       "3                            3.45   1480.0       0  \n",
       "4                            2.93    735.0       0  \n",
       "..                            ...      ...     ...  \n",
       "173                          1.74    740.0       2  \n",
       "174                          1.56    750.0       2  \n",
       "175                          1.56    835.0       2  \n",
       "176                          1.62    840.0       2  \n",
       "177                          1.60    560.0       2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_data = load_wine(as_frame=True)\n",
    "\n",
    "wine_frame = wine_data.frame\n",
    "wine_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "733e17f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2375fd",
   "metadata": {},
   "source": [
    "# Breast Cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "370e492d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67c995c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _breast_cancer_dataset:\n",
      "\n",
      "Breast cancer wisconsin (diagnostic) dataset\n",
      "--------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry\n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "\n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        worst/largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n",
      "        10 is Radius SE, field 20 is Worst Radius.\n",
      "\n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ====== ======\n",
      "                                           Min    Max\n",
      "    ===================================== ====== ======\n",
      "    radius (mean):                        6.981  28.11\n",
      "    texture (mean):                       9.71   39.28\n",
      "    perimeter (mean):                     43.79  188.5\n",
      "    area (mean):                          143.5  2501.0\n",
      "    smoothness (mean):                    0.053  0.163\n",
      "    compactness (mean):                   0.019  0.345\n",
      "    concavity (mean):                     0.0    0.427\n",
      "    concave points (mean):                0.0    0.201\n",
      "    symmetry (mean):                      0.106  0.304\n",
      "    fractal dimension (mean):             0.05   0.097\n",
      "    radius (standard error):              0.112  2.873\n",
      "    texture (standard error):             0.36   4.885\n",
      "    perimeter (standard error):           0.757  21.98\n",
      "    area (standard error):                6.802  542.2\n",
      "    smoothness (standard error):          0.002  0.031\n",
      "    compactness (standard error):         0.002  0.135\n",
      "    concavity (standard error):           0.0    0.396\n",
      "    concave points (standard error):      0.0    0.053\n",
      "    symmetry (standard error):            0.008  0.079\n",
      "    fractal dimension (standard error):   0.001  0.03\n",
      "    radius (worst):                       7.93   36.04\n",
      "    texture (worst):                      12.02  49.54\n",
      "    perimeter (worst):                    50.41  251.2\n",
      "    area (worst):                         185.2  4254.0\n",
      "    smoothness (worst):                   0.071  0.223\n",
      "    compactness (worst):                  0.027  1.058\n",
      "    concavity (worst):                    0.0    1.252\n",
      "    concave points (worst):               0.0    0.291\n",
      "    symmetry (worst):                     0.156  0.664\n",
      "    fractal dimension (worst):            0.055  0.208\n",
      "    ===================================== ====== ======\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
      "     San Jose, CA, 1993.\n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n"
     ]
    }
   ],
   "source": [
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b0d8fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (114, 30) (455,) (114,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d284a3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93        43\n",
      "           1       0.99      0.93      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.95      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SDG Classifier\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a780184e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92        43\n",
      "           1       0.92      1.00      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.96      0.93      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4265920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.91      0.94        43\n",
      "           1       0.95      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=500)  # Increased max_iter for convergence, adjust as needed\n",
    "\n",
    "# Fit the model to your training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, pred))\n",
    "\n",
    "accuracy_score(y_test, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "57e5324f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py:122: PkgResourcesDeprecationWarning: 0.996-ko-0.9.2 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/pkg_resources/__init__.py:122: PkgResourcesDeprecationWarning: 0.996-ko-0.9.2 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.8; total time=  21.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.8; total time=   6.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.9; total time=  13.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1; total time=  18.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  10.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=  11.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=   8.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1; total time=  11.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=150, subsample=0.8; total time=   8.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=150, subsample=0.9; total time=  14.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=150, subsample=0.9; total time=   4.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=150, subsample=1; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=0.8; total time=   9.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=0.9; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=1; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=1; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.9; total time=  52.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.9; total time=  30.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1; total time=  28.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=150, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=150, subsample=0.9; total time=  10.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=150, subsample=0.9; total time=  20.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=150, subsample=1; total time=   9.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.8; total time=  10.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.8; total time=  20.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.9; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.9; total time=  27.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=1; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=  39.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   5.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1; total time=  13.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=150, subsample=0.8; total time=  31.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=150, subsample=0.8; total time=  20.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=150, subsample=0.9; total time=  39.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=150, subsample=1; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=150, subsample=1; total time=  49.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.9; total time=  10.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1; total time=  19.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1; total time=   9.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1; total time=  22.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=150, subsample=0.8; total time=   7.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=150, subsample=0.9; total time=  34.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=150, subsample=1; total time=  10.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=150, subsample=1; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=0.8; total time=   3.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=0.9; total time=   7.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=1; total time=  25.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=1; total time=  15.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=  21.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.9; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.9; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1; total time=  35.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1; total time=   4.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=150, subsample=0.8; total time=  19.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=150, subsample=0.8; total time=  20.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=150, subsample=0.9; total time=   9.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=150, subsample=0.9; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=150, subsample=1; total time=  47.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=0.8; total time=  26.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.8; total time=  21.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.9; total time=   6.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=0.9; total time=  13.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=50, subsample=1; total time=   8.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  11.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=  10.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.9; total time=  18.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1; total time=  11.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=150, subsample=0.8; total time=   8.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=150, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=150, subsample=0.9; total time=  16.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=150, subsample=1; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=150, subsample=1; total time=  10.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=0.9; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=0.9; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=50, subsample=1; total time=   1.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8; total time=   5.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8; total time=  51.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.9; total time=  31.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1; total time=   2.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=150, subsample=0.8; total time=  27.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=150, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=150, subsample=0.9; total time=  19.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=150, subsample=1; total time=  11.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=150, subsample=1; total time=  19.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.8; total time=  19.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=0.9; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=1; total time=  19.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=50, subsample=1; total time=   9.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=  39.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.9; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1; total time=  10.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1; total time=  28.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=150, subsample=0.8; total time=  22.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=150, subsample=0.9; total time=   9.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=150, subsample=0.9; total time=  39.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=150, subsample=1; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.8; total time=  25.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.8; total time=  24.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.9; total time=  10.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=0.9; total time=  19.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=50, subsample=1; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.9; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.9; total time=   5.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1; total time=  16.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=150, subsample=0.8; total time=  16.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=150, subsample=0.8; total time=   7.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=150, subsample=0.9; total time=  22.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=150, subsample=0.9; total time=  15.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=150, subsample=1; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=0.8; total time=   4.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=0.9; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=0.9; total time=  16.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=50, subsample=1; total time=  16.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=  36.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.9; total time=   9.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1; total time=  31.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=150, subsample=0.8; total time=  39.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=150, subsample=0.9; total time=   9.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=150, subsample=1; total time=   0.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=150, subsample=1; total time=  28.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=0.8; total time=  25.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=0.8; total time=  28.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=0.9; total time=   7.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=50, subsample=1; total time=   3.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  38.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=  38.3s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/810304859.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Fit GridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Best parameters and best score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## GPT for XGBoost Hyperparameter Tuning\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import load_wine\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'subsample': [0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.8, 0.9, 1]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best score found: \", grid_search.best_score_)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "best_model = grid_search.best_estimator_\n",
    "predictions = best_model.predict(X)\n",
    "\n",
    "print(classification_report(y, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91bd8043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV] END C=46.92680899768591, class_weight=balanced, gamma=0.02026485043181491, kernel=linear; total time=   0.4s\n",
      "[CV] END C=91.29425537759532, class_weight=balanced, gamma=0.05902887476385446, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=91.29425537759532, class_weight=balanced, gamma=0.05902887476385446, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=91.29425537759532, class_weight=balanced, gamma=0.05902887476385446, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=5.983876860868068, class_weight=None, gamma=0.040602818331379076, kernel=linear; total time=   0.1s\n",
      "[CV] END C=5.983876860868068, class_weight=None, gamma=0.040602818331379076, kernel=linear; total time=   0.4s\n",
      "[CV] END C=123.12500617045903, class_weight=None, gamma=0.005806520264207263, kernel=linear; total time=   0.1s\n",
      "[CV] END C=123.12500617045903, class_weight=None, gamma=0.005806520264207263, kernel=linear; total time=   0.3s\n",
      "[CV] END C=178.6429543354675, class_weight=None, gamma=7.790692366582295e-05, kernel=linear; total time=   0.1s\n",
      "[CV] END C=178.6429543354675, class_weight=None, gamma=7.790692366582295e-05, kernel=linear; total time=   0.4s\n",
      "[CV] END C=56.55370667803366, class_weight=balanced, gamma=0.0743966187983319, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=56.55370667803366, class_weight=balanced, gamma=0.0743966187983319, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.023452872733866, class_weight=None, gamma=0.36402995536277605, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.023452872733866, class_weight=None, gamma=0.36402995536277605, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=60.893468785977504, class_weight=balanced, gamma=0.0963345676906832, kernel=linear; total time=   0.4s\n",
      "[CV] END C=72.20291550331277, class_weight=None, gamma=0.06287891005408561, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=72.20291550331277, class_weight=None, gamma=0.06287891005408561, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=93.533302064961, class_weight=balanced, gamma=0.05987451438464722, kernel=linear; total time=   0.1s\n",
      "[CV] END C=93.533302064961, class_weight=balanced, gamma=0.05987451438464722, kernel=linear; total time=   0.4s\n",
      "[CV] END C=297.3687793506931, class_weight=None, gamma=0.08284818388809986, kernel=linear; total time=   0.4s\n",
      "[CV] END C=297.3687793506931, class_weight=None, gamma=0.08284818388809986, kernel=linear; total time=   0.3s\n",
      "[CV] END C=58.009084258539836, class_weight=balanced, gamma=0.0941599970107014, kernel=linear; total time=   0.4s\n",
      "[CV] END C=58.009084258539836, class_weight=balanced, gamma=0.0941599970107014, kernel=linear; total time=   0.4s\n",
      "[CV] END C=108.62557985649804, class_weight=None, gamma=0.05536563610069876, kernel=linear; total time=   0.4s\n",
      "[CV] END C=108.62557985649804, class_weight=None, gamma=0.05536563610069876, kernel=linear; total time=   0.3s\n",
      "[CV] END C=79.12237979817975, class_weight=None, gamma=0.003181403459662207, kernel=linear; total time=   0.4s\n",
      "[CV] END C=149.22453771381407, class_weight=None, gamma=0.05027751757587391, kernel=linear; total time=   0.1s\n",
      "[CV] END C=91.1054412383275, class_weight=None, gamma=0.039532781771198434, kernel=linear; total time=   0.1s\n",
      "[CV] END C=91.1054412383275, class_weight=None, gamma=0.039532781771198434, kernel=linear; total time=   0.4s\n",
      "[CV] END C=21.813469463126744, class_weight=balanced, gamma=0.32486142702403464, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=21.813469463126744, class_weight=balanced, gamma=0.32486142702403464, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=49.21302917942186, class_weight=None, gamma=0.07758597359502013, kernel=linear; total time=   0.1s\n",
      "[CV] END C=49.21302917942186, class_weight=None, gamma=0.07758597359502013, kernel=linear; total time=   0.4s\n",
      "[CV] END C=44.12269995298701, class_weight=None, gamma=0.09340328151510838, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=44.12269995298701, class_weight=None, gamma=0.09340328151510838, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.189814782062886, class_weight=balanced, gamma=0.01806432927428257, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.189814782062886, class_weight=balanced, gamma=0.01806432927428257, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.189814782062886, class_weight=balanced, gamma=0.01806432927428257, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=433.41463395873194, class_weight=None, gamma=0.05023310023986688, kernel=linear; total time=   0.1s\n",
      "[CV] END C=433.41463395873194, class_weight=None, gamma=0.05023310023986688, kernel=linear; total time=   0.4s\n",
      "[CV] END C=433.41463395873194, class_weight=None, gamma=0.05023310023986688, kernel=linear; total time=   0.4s\n",
      "[CV] END C=130.5662908787999, class_weight=balanced, gamma=0.0931302787993573, kernel=linear; total time=   0.4s\n",
      "[CV] END C=130.5662908787999, class_weight=balanced, gamma=0.0931302787993573, kernel=linear; total time=   0.3s\n",
      "[CV] END C=37.24883504103537, class_weight=None, gamma=0.10963818478042127, kernel=linear; total time=   0.4s\n",
      "[CV] END C=101.488935892602, class_weight=balanced, gamma=0.08238105204914145, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=73.9678838777267, class_weight=balanced, gamma=0.011753523977362951, kernel=linear; total time=   0.4s\n",
      "[CV] END C=11.416743519186584, class_weight=balanced, gamma=0.22610938409423673, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=17.580835850006284, class_weight=None, gamma=0.09791081965694842, kernel=linear; total time=   0.4s\n",
      "[CV] END C=17.580835850006284, class_weight=None, gamma=0.09791081965694842, kernel=linear; total time=   0.4s\n",
      "[CV] END C=20.649547635863907, class_weight=None, gamma=0.21479795930605772, kernel=linear; total time=   0.4s\n",
      "[CV] END C=20.649547635863907, class_weight=None, gamma=0.21479795930605772, kernel=linear; total time=   0.4s\n",
      "[CV] END C=11.659216012385084, class_weight=balanced, gamma=0.10432441512843443, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=11.659216012385084, class_weight=balanced, gamma=0.10432441512843443, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=170.3829726638945, class_weight=balanced, gamma=0.036352874647320794, kernel=linear; total time=   0.1s\n",
      "[CV] END C=170.3829726638945, class_weight=balanced, gamma=0.036352874647320794, kernel=linear; total time=   0.4s\n",
      "[CV] END C=71.48761593385558, class_weight=balanced, gamma=0.06632582801058078, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=71.48761593385558, class_weight=balanced, gamma=0.06632582801058078, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=12.768039154564681, class_weight=None, gamma=0.0279879951307172, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=12.768039154564681, class_weight=None, gamma=0.0279879951307172, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=12.768039154564681, class_weight=None, gamma=0.0279879951307172, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=39.03838028814744, class_weight=None, gamma=0.08166761973316981, kernel=linear; total time=   0.1s\n",
      "[CV] END C=45.197449896696234, class_weight=balanced, gamma=0.02929162990591085, kernel=linear; total time=   0.1s\n",
      "[CV] END C=45.197449896696234, class_weight=balanced, gamma=0.02929162990591085, kernel=linear; total time=   0.5s\n",
      "[CV] END C=45.197449896696234, class_weight=balanced, gamma=0.02929162990591085, kernel=linear; total time=   0.4s\n",
      "[CV] END C=29.006129499220528, class_weight=None, gamma=0.12457347888471555, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=33.52496763810817, class_weight=balanced, gamma=0.031031086901916773, kernel=linear; total time=   0.4s\n",
      "[CV] END C=69.85196328296385, class_weight=None, gamma=0.0033609249124939896, kernel=linear; total time=   0.1s\n",
      "[CV] END C=46.92680899768591, class_weight=balanced, gamma=0.02026485043181491, kernel=linear; total time=   5.1s\n",
      "[CV] END C=91.29425537759532, class_weight=balanced, gamma=0.05902887476385446, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=91.29425537759532, class_weight=balanced, gamma=0.05902887476385446, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=91.29425537759532, class_weight=balanced, gamma=0.05902887476385446, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=5.983876860868068, class_weight=None, gamma=0.040602818331379076, kernel=linear; total time=   6.2s\n",
      "[CV] END C=5.983876860868068, class_weight=None, gamma=0.040602818331379076, kernel=linear; total time=   6.5s\n",
      "[CV] END C=170.3829726638945, class_weight=balanced, gamma=0.036352874647320794, kernel=linear; total time=   0.3s\n",
      "[CV] END C=71.48761593385558, class_weight=balanced, gamma=0.06632582801058078, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=39.03838028814744, class_weight=None, gamma=0.08166761973316981, kernel=linear; total time=   0.4s\n",
      "[CV] END C=39.03838028814744, class_weight=None, gamma=0.08166761973316981, kernel=linear; total time=   0.4s\n",
      "[CV] END C=29.006129499220528, class_weight=None, gamma=0.12457347888471555, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=29.006129499220528, class_weight=None, gamma=0.12457347888471555, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=33.52496763810817, class_weight=balanced, gamma=0.031031086901916773, kernel=linear; total time=   0.1s\n",
      "[CV] END C=33.52496763810817, class_weight=balanced, gamma=0.031031086901916773, kernel=linear; total time=   0.4s\n",
      "[CV] END C=69.85196328296385, class_weight=None, gamma=0.0033609249124939896, kernel=linear; total time=   0.2s\n",
      "[CV] END C=69.85196328296385, class_weight=None, gamma=0.0033609249124939896, kernel=linear; total time=   0.1s\n",
      "[CV] END C=46.92680899768591, class_weight=balanced, gamma=0.02026485043181491, kernel=linear; total time=   4.9s\n",
      "[CV] END C=46.92680899768591, class_weight=balanced, gamma=0.02026485043181491, kernel=linear; total time=   9.0s\n",
      "[CV] END C=5.983876860868068, class_weight=None, gamma=0.040602818331379076, kernel=linear; total time=  10.1s\n",
      "[CV] END C=123.12500617045903, class_weight=None, gamma=0.005806520264207263, kernel=linear; total time=  12.2s\n",
      "[CV] END C=123.12500617045903, class_weight=None, gamma=0.005806520264207263, kernel=linear; total time=  13.9s\n",
      "[CV] END C=178.6429543354675, class_weight=None, gamma=7.790692366582295e-05, kernel=linear; total time=  18.7s\n",
      "[CV] END C=20.261142283225706, class_weight=None, gamma=0.0945856422396747, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=20.261142283225706, class_weight=None, gamma=0.0945856422396747, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=20.261142283225706, class_weight=None, gamma=0.0945856422396747, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=56.55370667803366, class_weight=balanced, gamma=0.0743966187983319, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=56.55370667803366, class_weight=balanced, gamma=0.0743966187983319, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=56.55370667803366, class_weight=balanced, gamma=0.0743966187983319, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.023452872733866, class_weight=None, gamma=0.36402995536277605, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.023452872733866, class_weight=None, gamma=0.36402995536277605, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.023452872733866, class_weight=None, gamma=0.36402995536277605, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=60.893468785977504, class_weight=balanced, gamma=0.0963345676906832, kernel=linear; total time=  12.4s\n",
      "[CV] END C=72.20291550331277, class_weight=None, gamma=0.06287891005408561, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=72.20291550331277, class_weight=None, gamma=0.06287891005408561, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=72.20291550331277, class_weight=None, gamma=0.06287891005408561, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=93.533302064961, class_weight=balanced, gamma=0.05987451438464722, kernel=linear; total time=  13.0s\n",
      "[CV] END C=297.3687793506931, class_weight=None, gamma=0.08284818388809986, kernel=linear; total time=  18.2s\n",
      "[CV] END C=297.3687793506931, class_weight=None, gamma=0.08284818388809986, kernel=linear; total time=  17.8s\n",
      "[CV] END C=58.009084258539836, class_weight=balanced, gamma=0.0941599970107014, kernel=linear; total time=  13.5s\n",
      "[CV] END C=58.009084258539836, class_weight=balanced, gamma=0.0941599970107014, kernel=linear; total time=   6.5s\n",
      "[CV] END C=3.4993721443479084, class_weight=None, gamma=0.04960365360493639, kernel=linear; total time=   2.8s\n",
      "[CV] END C=3.4993721443479084, class_weight=None, gamma=0.04960365360493639, kernel=linear; total time=   4.1s\n",
      "[CV] END C=108.62557985649804, class_weight=None, gamma=0.05536563610069876, kernel=linear; total time=  11.6s\n",
      "[CV] END C=79.12237979817975, class_weight=None, gamma=0.003181403459662207, kernel=linear; total time=  10.1s\n",
      "[CV] END C=79.12237979817975, class_weight=None, gamma=0.003181403459662207, kernel=linear; total time=   4.3s\n",
      "[CV] END C=149.22453771381407, class_weight=None, gamma=0.05027751757587391, kernel=linear; total time=  12.2s\n",
      "[CV] END C=149.22453771381407, class_weight=None, gamma=0.05027751757587391, kernel=linear; total time=  15.1s\n",
      "[CV] END C=91.1054412383275, class_weight=None, gamma=0.039532781771198434, kernel=linear; total time=   4.3s\n",
      "[CV] END C=21.813469463126744, class_weight=balanced, gamma=0.32486142702403464, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=21.813469463126744, class_weight=balanced, gamma=0.32486142702403464, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=21.813469463126744, class_weight=balanced, gamma=0.32486142702403464, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=49.21302917942186, class_weight=None, gamma=0.07758597359502013, kernel=linear; total time=  19.7s\n",
      "[CV] END C=44.12269995298701, class_weight=None, gamma=0.09340328151510838, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=44.12269995298701, class_weight=None, gamma=0.09340328151510838, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=44.12269995298701, class_weight=None, gamma=0.09340328151510838, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.189814782062886, class_weight=balanced, gamma=0.01806432927428257, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.189814782062886, class_weight=balanced, gamma=0.01806432927428257, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=15.189814782062886, class_weight=balanced, gamma=0.01806432927428257, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=433.41463395873194, class_weight=None, gamma=0.05023310023986688, kernel=linear; total time=  13.5s\n",
      "[CV] END C=433.41463395873194, class_weight=None, gamma=0.05023310023986688, kernel=linear; total time=  18.9s\n",
      "[CV] END C=130.5662908787999, class_weight=balanced, gamma=0.0931302787993573, kernel=linear; total time=   7.6s\n",
      "[CV] END C=130.5662908787999, class_weight=balanced, gamma=0.0931302787993573, kernel=linear; total time=  21.1s\n",
      "[CV] END C=37.24883504103537, class_weight=None, gamma=0.10963818478042127, kernel=linear; total time=  10.9s\n",
      "[CV] END C=73.9678838777267, class_weight=balanced, gamma=0.011753523977362951, kernel=linear; total time=   5.0s\n",
      "[CV] END C=73.9678838777267, class_weight=balanced, gamma=0.011753523977362951, kernel=linear; total time=  10.5s\n",
      "[CV] END C=17.580835850006284, class_weight=None, gamma=0.09791081965694842, kernel=linear; total time=   4.1s\n",
      "[CV] END C=100.34941905205311, class_weight=None, gamma=0.060978918158816224, kernel=linear; total time=  21.2s\n",
      "[CV] END C=20.649547635863907, class_weight=None, gamma=0.21479795930605772, kernel=linear; total time=  15.4s\n",
      "[CV] END C=164.73482913035662, class_weight=None, gamma=0.04405191413413165, kernel=linear; total time=  14.1s\n",
      "[CV] END C=11.659216012385084, class_weight=balanced, gamma=0.10432441512843443, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=11.659216012385084, class_weight=balanced, gamma=0.10432441512843443, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=11.659216012385084, class_weight=balanced, gamma=0.10432441512843443, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=170.3829726638945, class_weight=balanced, gamma=0.036352874647320794, kernel=linear; total time=  12.1s\n",
      "[CV] END C=170.3829726638945, class_weight=balanced, gamma=0.036352874647320794, kernel=linear; total time=  11.3s\n",
      "[CV] END C=71.48761593385558, class_weight=balanced, gamma=0.06632582801058078, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=71.48761593385558, class_weight=balanced, gamma=0.06632582801058078, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=71.48761593385558, class_weight=balanced, gamma=0.06632582801058078, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=12.768039154564681, class_weight=None, gamma=0.0279879951307172, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=12.768039154564681, class_weight=None, gamma=0.0279879951307172, kernel=rbf; total time=   0.0s\n",
      "Best parameters found:  {'C': 73.9678838777267, 'class_weight': 'balanced', 'gamma': 0.011753523977362951, 'kernel': 'linear'}\n",
      "Best score:  0.9648251423260138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93        43\n",
      "           1       0.97      0.94      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.95      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "[CV] END C=123.12500617045903, class_weight=None, gamma=0.005806520264207263, kernel=linear; total time=  22.8s\n",
      "[CV] END C=178.6429543354675, class_weight=None, gamma=7.790692366582295e-05, kernel=linear; total time=  15.6s\n",
      "[CV] END C=178.6429543354675, class_weight=None, gamma=7.790692366582295e-05, kernel=linear; total time=  15.3s\n",
      "[CV] END C=60.893468785977504, class_weight=balanced, gamma=0.0963345676906832, kernel=linear; total time=   5.0s\n",
      "[CV] END C=60.893468785977504, class_weight=balanced, gamma=0.0963345676906832, kernel=linear; total time=  10.2s\n",
      "[CV] END C=93.533302064961, class_weight=balanced, gamma=0.05987451438464722, kernel=linear; total time=   5.9s\n",
      "[CV] END C=93.533302064961, class_weight=balanced, gamma=0.05987451438464722, kernel=linear; total time=  10.6s\n",
      "[CV] END C=297.3687793506931, class_weight=None, gamma=0.08284818388809986, kernel=linear; total time=  14.9s\n",
      "[CV] END C=36.32878599687583, class_weight=balanced, gamma=0.02625262508953778, kernel=linear; total time=   3.9s\n",
      "[CV] END C=36.32878599687583, class_weight=balanced, gamma=0.02625262508953778, kernel=linear; total time=   4.9s\n",
      "[CV] END C=36.32878599687583, class_weight=balanced, gamma=0.02625262508953778, kernel=linear; total time=  12.4s\n",
      "[CV] END C=58.009084258539836, class_weight=balanced, gamma=0.0941599970107014, kernel=linear; total time=   5.1s\n",
      "[CV] END C=3.4993721443479084, class_weight=None, gamma=0.04960365360493639, kernel=linear; total time=  17.2s\n",
      "[CV] END C=108.62557985649804, class_weight=None, gamma=0.05536563610069876, kernel=linear; total time=   4.1s\n",
      "[CV] END C=108.62557985649804, class_weight=None, gamma=0.05536563610069876, kernel=linear; total time=  13.5s\n",
      "[CV] END C=79.12237979817975, class_weight=None, gamma=0.003181403459662207, kernel=linear; total time=  10.9s\n",
      "[CV] END C=149.22453771381407, class_weight=None, gamma=0.05027751757587391, kernel=linear; total time=  13.1s\n",
      "[CV] END C=91.1054412383275, class_weight=None, gamma=0.039532781771198434, kernel=linear; total time=  11.0s\n",
      "[CV] END C=91.1054412383275, class_weight=None, gamma=0.039532781771198434, kernel=linear; total time=  11.5s\n",
      "[CV] END C=49.21302917942186, class_weight=None, gamma=0.07758597359502013, kernel=linear; total time=   4.2s\n",
      "[CV] END C=49.21302917942186, class_weight=None, gamma=0.07758597359502013, kernel=linear; total time=  14.1s\n",
      "[CV] END C=433.41463395873194, class_weight=None, gamma=0.05023310023986688, kernel=linear; total time=   9.2s\n",
      "[CV] END C=0.5537420375935475, class_weight=balanced, gamma=0.022169760231351216, kernel=linear; total time=   3.6s\n",
      "[CV] END C=0.5537420375935475, class_weight=balanced, gamma=0.022169760231351216, kernel=linear; total time=   1.0s\n",
      "[CV] END C=0.5537420375935475, class_weight=balanced, gamma=0.022169760231351216, kernel=linear; total time=   1.0s\n",
      "[CV] END C=130.5662908787999, class_weight=balanced, gamma=0.0931302787993573, kernel=linear; total time=  21.0s\n",
      "[CV] END C=44.38926724162325, class_weight=balanced, gamma=0.24646297296191846, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=44.38926724162325, class_weight=balanced, gamma=0.24646297296191846, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=44.38926724162325, class_weight=balanced, gamma=0.24646297296191846, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=97.63011917123741, class_weight=None, gamma=0.0100273605386305, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=97.63011917123741, class_weight=None, gamma=0.0100273605386305, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=97.63011917123741, class_weight=None, gamma=0.0100273605386305, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=37.24883504103537, class_weight=None, gamma=0.10963818478042127, kernel=linear; total time=  13.6s\n",
      "[CV] END C=37.24883504103537, class_weight=None, gamma=0.10963818478042127, kernel=linear; total time=  10.3s\n",
      "[CV] END C=101.488935892602, class_weight=balanced, gamma=0.08238105204914145, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=101.488935892602, class_weight=balanced, gamma=0.08238105204914145, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=101.488935892602, class_weight=balanced, gamma=0.08238105204914145, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=12.73723936155019, class_weight=None, gamma=0.18899049047300434, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=12.73723936155019, class_weight=None, gamma=0.18899049047300434, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=12.73723936155019, class_weight=None, gamma=0.18899049047300434, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=82.38874948858287, class_weight=balanced, gamma=0.029580608003427725, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=82.38874948858287, class_weight=balanced, gamma=0.029580608003427725, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=82.38874948858287, class_weight=balanced, gamma=0.029580608003427725, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=73.9678838777267, class_weight=balanced, gamma=0.011753523977362951, kernel=linear; total time=  12.7s\n",
      "[CV] END C=11.416743519186584, class_weight=balanced, gamma=0.22610938409423673, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=11.416743519186584, class_weight=balanced, gamma=0.22610938409423673, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=11.416743519186584, class_weight=balanced, gamma=0.22610938409423673, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=37.73967087950999, class_weight=None, gamma=0.11891370222133257, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=37.73967087950999, class_weight=None, gamma=0.11891370222133257, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=37.73967087950999, class_weight=None, gamma=0.11891370222133257, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=28.673882298329833, class_weight=balanced, gamma=0.07761833954623698, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=28.673882298329833, class_weight=balanced, gamma=0.07761833954623698, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=28.673882298329833, class_weight=balanced, gamma=0.07761833954623698, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=25.980515691739402, class_weight=None, gamma=0.09134584129003875, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=25.980515691739402, class_weight=None, gamma=0.09134584129003875, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=25.980515691739402, class_weight=None, gamma=0.09134584129003875, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=17.580835850006284, class_weight=None, gamma=0.09791081965694842, kernel=linear; total time=   4.9s\n",
      "[CV] END C=17.580835850006284, class_weight=None, gamma=0.09791081965694842, kernel=linear; total time=   9.1s\n",
      "[CV] END C=100.34941905205311, class_weight=None, gamma=0.060978918158816224, kernel=linear; total time=   3.9s\n",
      "[CV] END C=100.34941905205311, class_weight=None, gamma=0.060978918158816224, kernel=linear; total time=  12.8s\n",
      "[CV] END C=20.649547635863907, class_weight=None, gamma=0.21479795930605772, kernel=linear; total time=   2.9s\n",
      "[CV] END C=20.649547635863907, class_weight=None, gamma=0.21479795930605772, kernel=linear; total time=   5.7s\n",
      "[CV] END C=164.73482913035662, class_weight=None, gamma=0.04405191413413165, kernel=linear; total time=  19.6s\n",
      "[CV] END C=164.73482913035662, class_weight=None, gamma=0.04405191413413165, kernel=linear; total time=  15.3s\n",
      "[CV] END C=170.3829726638945, class_weight=balanced, gamma=0.036352874647320794, kernel=linear; total time=  19.0s\n",
      "[CV] END C=39.03838028814744, class_weight=None, gamma=0.08166761973316981, kernel=linear; total time=  10.5s\n",
      "[CV] END C=45.197449896696234, class_weight=balanced, gamma=0.02929162990591085, kernel=linear; total time=   9.3s\n",
      "[CV] END C=33.52496763810817, class_weight=balanced, gamma=0.031031086901916773, kernel=linear; total time=   5.1s\n",
      "[CV] END C=33.52496763810817, class_weight=balanced, gamma=0.031031086901916773, kernel=linear; total time=   8.6s\n",
      "[CV] END C=69.85196328296385, class_weight=None, gamma=0.0033609249124939896, kernel=linear; total time=   4.3s\n",
      "[CV] END C=12.768039154564681, class_weight=None, gamma=0.0279879951307172, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=39.03838028814744, class_weight=None, gamma=0.08166761973316981, kernel=linear; total time=   8.9s\n",
      "[CV] END C=39.03838028814744, class_weight=None, gamma=0.08166761973316981, kernel=linear; total time=   5.0s\n",
      "[CV] END C=45.197449896696234, class_weight=balanced, gamma=0.02929162990591085, kernel=linear; total time=   4.7s\n",
      "[CV] END C=45.197449896696234, class_weight=balanced, gamma=0.02929162990591085, kernel=linear; total time=   5.2s\n",
      "[CV] END C=29.006129499220528, class_weight=None, gamma=0.12457347888471555, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=29.006129499220528, class_weight=None, gamma=0.12457347888471555, kernel=rbf; total time=   0.0s\n",
      "[CV] END C=29.006129499220528, class_weight=None, gamma=0.12457347888471555, kernel=rbf; total time=   0.1s\n",
      "[CV] END C=33.52496763810817, class_weight=balanced, gamma=0.031031086901916773, kernel=linear; total time=  11.7s\n",
      "[CV] END C=69.85196328296385, class_weight=None, gamma=0.0033609249124939896, kernel=linear; total time=   8.8s\n",
      "[CV] END C=69.85196328296385, class_weight=None, gamma=0.0033609249124939896, kernel=linear; total time=   6.1s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import expon, randint\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "param_distributions = {\n",
    "    'C': expon(scale=100),  # Continuous distribution\n",
    "    'gamma': expon(scale=.1),  # Continuous distribution\n",
    "    'kernel': ['rbf', 'linear'],  # Categorical distribution\n",
    "    'class_weight': ['balanced', None],  # Categorical distribution\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(model, param_distributions, n_iter=50, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameter set found\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "\n",
    "# Best score\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# Optional: use the best estimator directly\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "pred = best_model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbc6c9",
   "metadata": {},
   "source": [
    "## Breast Cancer 회고\n",
    "\n",
    "- Random Forest 가 가장 recall 이 높게 나왔다\n",
    "- GridSearch 는 너무 오래 걸린다. 회고를 다 적고 다른 거 할 때까지 돌아간다. n_jobs 도 -1 인데! 앞으로는 Random Search 해야지\n",
    "- 결국 keyboard interrupt 했다. Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
    "- 729 번이나 피팅을 해야하다니! 다시 랜덤서치로 돌렸다. \n",
    "- print(random_search.best_params_) 로 했는데 그게 중간에 떠서 다 안끝난 줄 알았다. F1 macro 0.94 라 애초에 모델(SVC) 에서 정확도 제일 높은 걸 선택한 다음 하이퍼파람 튜닝을 해야겠다 (RF 0.96)\n",
    "- 여기서 metric 은 recall 을 사용한다. 암 진단이고 binary classification 이라 지표가 헷갈리진 않았다.\n",
    "- multi-classification 에서 어떤 지표를 써야하냐고 지피티에게 물었더니 f1 등 골고루 보되, 특정 class 에 대한 분류 예측력이 유독 떨어지는지를 보라고 한다\n",
    "- class imbalance : 특정 클래스에 속하는 데이터셋 개수가 적을 때. 예) 카드 거래 사기 Ture 가 0.1% 비율일 때\n",
    "- 매크로 평균: 모든 클래스에 같은 비율을 적용\n",
    "- 마이크로 평균: class imbalance 가 있을 때 좋음. 전체에서 맞은 문제 세기\n",
    "- 데이터가 나온 웹사이트를 보니 RandomForest / XGBoost 가 가장 예측을 잘한다고 베이스라인이 써있었다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
