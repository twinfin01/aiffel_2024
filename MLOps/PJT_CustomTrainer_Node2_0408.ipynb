{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d658a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9dee994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete bad images\n",
    "\n",
    "data_path = '/aiffel/aiffel/CustomM/model-fit/data/30vnfoods/'\n",
    "train_path = data_path + 'Train/'\n",
    "test_path = data_path + 'Test/'\n",
    "\n",
    "for path in [train_path, test_path]:\n",
    "    classes = os.listdir(path)\n",
    "    \n",
    "    for food in classes:\n",
    "        food_path = os.path.join(path, food)\n",
    "        images = os.listdir(food_path)\n",
    "        \n",
    "        for image in images:\n",
    "            with open(os.path.join(food_path, image), 'rb') as f:\n",
    "                bytes = f.read()\n",
    "            if bytes[:3] != b'\\xff\\xd8\\xff':\n",
    "                print(os.path.join(food_path, image))\n",
    "                os.remove(os.path.join(food_path, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d07a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Banh mi',\n",
       " 'Pho',\n",
       " 'Banh cuon',\n",
       " 'Banh xeo',\n",
       " 'Bun bo Hue',\n",
       " 'Bun dau mam tom',\n",
       " 'Chao long',\n",
       " 'Banh khot',\n",
       " 'Com tam',\n",
       " 'Bun rieu']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = os.listdir(train_path)\n",
    "train_len = 0\n",
    "\n",
    "for food in classes:\n",
    "    food_path = os.path.join(train_path, food)\n",
    "    images = os.listdir(food_path)\n",
    "    \n",
    "    train_len += len(images)\n",
    "    \n",
    "str(train_len)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fdb5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "\n",
    "class_names = ['Banh mi', 'Pho', 'Banh cuon', 'Banh xeo', 'Bun bo Hue']\n",
    "               \n",
    "\n",
    "\n",
    "def process_path(file_path, class_names=class_names, img_shape=(224, 224)):\n",
    "    \n",
    "    label = tf.strings.split(file_path, os.path.sep)[-2]\n",
    "    label = tf.argmax(label == class_names)\n",
    "    \n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, img_shape)\n",
    "    img = img / 255.0\n",
    "    \n",
    "    return img, label\n",
    "\n",
    "\n",
    "\n",
    "def prepare_for_training(ds, epochs=5, batch_size=32, cache=True, shuffle_buffer_size=100):\n",
    "\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size = shuffle_buffer_size)\n",
    "    ds = ds.repeat(epochs)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data_path, class_names, batch_size=32):\n",
    "    all_paths = []\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        class_files = [os.path.join(class_path, fname) for fname in os.listdir(class_path)]\n",
    "        all_paths.extend(class_files)\n",
    "\n",
    "    # Shuffle the list to mix up class distributions\n",
    "    np.random.shuffle(all_paths)\n",
    "\n",
    "    list_ds = tf.data.Dataset.from_tensor_slices(all_paths)\n",
    "    list_ds = list_ds.map(lambda x: process_path(x, class_names),\n",
    "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return prepare_for_training(list_ds, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "111fdae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "    \n",
    "    \n",
    "    def __init__(self, num_classes=10, freeze=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.base_model = EfficientNetB0(include_top=False,\n",
    "                                        input_shape=(224, 224, 3), weights='imagenet')\n",
    "        \n",
    "        if freeze:\n",
    "            for layer in self.base_model.layers:\n",
    "                layer.trainable = False\n",
    "                \n",
    "        self.global_avg_layer = layers.GlobalAveragePooling2D()\n",
    "        self.output_layer = layers.Dense(num_classes, activation='softmax')\n",
    "    \n",
    "        \n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.base_model(inputs, training=training)\n",
    "        x = self.global_avg_layer(x)\n",
    "        return self.output_layer(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f122982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom trainer\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, epochs, batch, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch = batch\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def train(self, train_dataset, train_metric):\n",
    "            \n",
    "        progbar = tqdm(total=len(train_dataset) * self.epochs, miniters=1, mininterval=0.1)\n",
    "\n",
    "            \n",
    "        for epoch in range(self.epochs):\n",
    "            print('\\nStart of epoch %d' % (epoch +1,))\n",
    "            \n",
    "            \n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    logits = self.model(x_batch_train, training=True)\n",
    "                    loss_value = self.loss_fn(y_batch_train, logits)\n",
    "                grads = tape.gradient(loss_value, self.model.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "            \n",
    "                train_acc_metric.update_state(y_batch_train, logits)\n",
    "                progbar.update(1)\n",
    "                progbar.set_postfix(loss=loss_value.numpy(), epoch=epoch+1)\n",
    "\n",
    "            train_acc = train_acc_metric.result()\n",
    "            print('training acc over epoch: %.4f' % (float(train_acc),))\n",
    "            train_acc_metric.reset_states()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9b08ea4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08998abb79a409b8a09122caa246900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5031 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n",
      "training acc over epoch: 0.9132\n",
      "\n",
      "Start of epoch 2\n",
      "training acc over epoch: 0.9676\n",
      "\n",
      "Start of epoch 3\n",
      "training acc over epoch: 0.9774\n"
     ]
    }
   ],
   "source": [
    "# now train!\n",
    "\n",
    "train_path = \"/aiffel/aiffel/CustomM/model-fit/data/30vnfoods/Train\"\n",
    "\n",
    "epoch = 3\n",
    "batch = 16\n",
    "\n",
    "\n",
    "\n",
    "model = Model(num_classes=10)\n",
    "dataset = load_data(data_path=train_path, class_names=class_names, batch_size=batch)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "trainer = Trainer(model=model,\n",
    "                 epochs=epoch,\n",
    "                 batch=batch,\n",
    "                 \n",
    "                 loss_fn=loss_fn,\n",
    "                 optimizer=optimizer)\n",
    "\n",
    "trainer.train(train_dataset=dataset, \n",
    "             train_metric=train_acc_metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdf14126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/32\n",
      "28/32\n",
      "27/32\n",
      "29/32\n",
      "31/32\n",
      "26/32\n",
      "26/32\n",
      "30/32\n",
      "28/32\n",
      "28/32\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "\n",
    "test_ds = load_data(data_path=test_path, class_names=class_names)\n",
    "\n",
    "for step_train, (x_batch_train, y_batch_train) in enumerate(test_ds.take(10)):\n",
    "    prediction = model(x_batch_train)\n",
    "    \n",
    "    correct_predictions = tf.equal(y_batch_train, tf.argmax(prediction, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n",
    "    \n",
    "    print(\"{}/{}\".format(tf.reduce_sum(tf.cast(correct_predictions, tf.int32)).numpy(),\n",
    "                         y_batch_train.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7537f38",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- 정확도는 80-90 후반대로 높게 나왔다\n",
    "- 사실 코드를 정확하게 이해하고 직접 짜지 못해서 GPT 의 하드캐리를 받았는데, 여기저기 문제가 생겨 계속 에러가 났다\n",
    "- 트레이닝 시간이 너무 오래걸려서 결국 데이터셋/클래스를 반으로 뚝 자르고 에포크도 3개만 돌렸다\n",
    "- GPT 가 간단히 해결해 준 것들도 있고, 전혀 딴소리만 해서 내가 이건가? 하고 때려맞췄는데 맞았던 것도 있고, 계속 답변이 바뀌어서 클로드랑 경쟁시켰던 것도 있는데, 결론적으로 잘 작동해서 기쁘다!\n",
    "- 이런 게 문제 해결의 즐거움일까? 지피티가 어서 더 똑똑해졌으면!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9102da62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
